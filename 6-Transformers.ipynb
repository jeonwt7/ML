{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Week 6 Assignment.ipynb","provenance":[{"file_id":"16bWvhy2TrtRkg_vhloC-LV1ALlnd2ckW","timestamp":1635564482677}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CnFfAowB3kBQ"},"source":["# Week 6ï¼š Transformers\n","\n","This week's coding assignment will be short. You will be implementing a function that computes multihead attention for a sequence of input tokens, however, you will not build or train the rest of the transformer due to time and compute constraints. Instead, you will walk yourself through another notebook that fine-tunes BERT for text classification and answer some questions about it.\n"]},{"cell_type":"markdown","metadata":{"id":"NGnw59Bi8bLf"},"source":["### Part I: Self Attention\n","For this part, you will implement self attention, the core mechanism in transformers, in a simplified setting. Given the input token sequence and projection matrices, your task is to write a function that computes the transformed sequence after applying self-attention. In practice, you would use autograd libraries like tensorflow or pytroch, but since you won't be training the model here, numpy will suffice. Don't worry about training -- your only job is to apply the layer.\n"]},{"cell_type":"markdown","metadata":{"id":"J4a1fr1dBAao"},"source":["Let's begin by defining the components. We will just use random matrices for the weights since we won't be able to train them. Once again, don't worry about optimization and focus on implementing the inference function, assuming that these are the weight matrices we get. "]},{"cell_type":"code","metadata":{"id":"-ZJC4g8ic2Ta","executionInfo":{"status":"ok","timestamp":1635572160760,"user_tz":420,"elapsed":152,"user":{"displayName":"Wootae Jeon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl4SKlzSu1Hl1zJoxXC4i2tUYG0XC4OZ5Eso8p=s64","userId":"06138653401797949709"}}},"source":["# Mount your drive before running this. the easiest way to do this is:\n","# click the Files icon on the left of this page -> mount drive (third icon)\n","\n","# modify this as needed\n","path = '/content/drive/MyDrive/Colab Notebooks/test_case/'\n","\n","import numpy as np\n","\n","d_qkv = 32    # dimension of our query, key, and value vectors\n","d_model = 256 # dimension of our model\n","input_len = 32\n","\n","# the input x consists of \n","# input_len tokens \n","# each represented by a vector of dimension d_model\n","x = np.load(path+'x.npy')\n","\n","# projection matrices to query, keys, and values\n","W_q = np.load(path+'W_q.npy')\n","W_k = np.load(path+'W_k.npy')\n","W_v = np.load(path+'W_v.npy')\n","\n","# final projection\n","W_o = np.load(path+'W_o.npy')"],"execution_count":67,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"emdwGgyYDG7B"},"source":["Now let's implement the attention layer. You will be doing a lot of matrix multiplications, so to save some pain thinking about how to reshape and transpose them, I recommend checking out the [einsum function](https://rockt.github.io/2018/04/30/einsum). \n","\n","![](https://www.researchgate.net/publication/350311050/figure/fig2/AS:1004363044642817@1616470218633/Transformers-Scaled-Dot-Product-Attention-and-Multi-Head-Attention-From-Vas-17.ppm)\n","\n","You will implement the computation in the left diagram. However, first you want to obtain the queries, keys, and values by projecting each token by the respective matrix. So if $x_i$ is token i's vector, $\\text{query}_i = W_qx_i$. For efficiency, try to avoid computing these projections individually and instead compute a matrix $Q$ such that  $Q[i] = \\text{query}_i$. (similarly for keys and values)\n","\n","$$\n","\\alpha_{ij} = \\text{softmax}(\\frac{\\text{(query}_i)^T(\\text{key}_j)}{\\sqrt{d_{qkv}}}) \\hspace{5mm}\n","$$\n","\n","Then you will compute the attention weights according to the above formula. In words, the attention of token $i$ on token $j$ is equal to the dot product between $\\text{query}_i$ and $\\text{key}_j$, scaled down by a factor of $\\sqrt{d_{qkv}}$. Remember to take the softmax at the end to normalize the attention weights. \n","\n","$$\n","\\text{out}_i = \\sum_j \\alpha_{ij} \\cdot \\text{value}_j\n","$$\n","\n","Now, apply the attention weights by computing a weighted sum of the values. \n","Finally, project back to the dimension of the model using $y_i = W_o \\text{out}_i$. Here, $y_i$ is the result of token $x_i$ after applying attention, which has queried for and combined information from other tokens (thus \"contextualized\").\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C2dRFXGcP4d2","executionInfo":{"status":"ok","timestamp":1635572161881,"user_tz":420,"elapsed":201,"user":{"displayName":"Wootae Jeon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl4SKlzSu1Hl1zJoxXC4i2tUYG0XC4OZ5Eso8p=s64","userId":"06138653401797949709"}},"outputId":"ef3cb967-2540-4203-9881-cd717788e7bc"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"Zevqg1t__kHS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635572163195,"user_tz":420,"elapsed":3,"user":{"displayName":"Wootae Jeon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl4SKlzSu1Hl1zJoxXC4i2tUYG0XC4OZ5Eso8p=s64","userId":"06138653401797949709"}},"outputId":"f7743757-da1c-45c8-f2f8-a523ef101275"},"source":["print(x.shape)\n","print(W_q.shape)\n","print(W_k.shape)\n","print(W_v.shape)\n","print(W_o.shape)\n","\n","def softmax(x):\n","    max = np.max(x, axis=1, keepdims=True)\n","    e_x = np.exp(x - max)\n","    sum = np.sum(e_x, axis=1, keepdims=True)\n","    f_x = e_x / sum \n","    return f_x\n","\n","def apply_self_attention(x, W_q, W_k, W_v, W_o):\n","  \n","  ### YOUR CODE HERE\n","  # find Q, K, V\n","  Q = x @ W_q # (32, 32)\n","  K = x @ W_k # (32, 32)\n","  V = x @ W_v # (32, 32)\n","  # compute attention weights\n","  Aij = Q @ K.T / (d_qkv ** 0.5) # (32, 32)\n","  A = softmax(Aij) # (32, 32)\n","  # apply attention to values\n","  weighted = np.einsum('ij,jk->jik', A, V) # (32, 32, 32)\n","  out = weighted.sum(axis=0) # (32, 32)\n","  # final projection\n","  return out @ W_o\n","\n","# expected output\n","y_ = np.load(path + 'y.npy')\n","y = apply_self_attention(x, W_q, W_k, W_v, W_o)\n","assert np.allclose(y, y_)"],"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["(32, 256)\n","(256, 32)\n","(256, 32)\n","(256, 32)\n","(32, 256)\n"]}]},{"cell_type":"markdown","metadata":{"id":"JqRYX2rugYAv"},"source":["Now, let's increase the complexity and implement multihead attention. Instead of having one matrix per projection, you will have n_head sets of $\\{W_q, W_k, W_v\\}$ that each represent a version of self-attention running in parallel. In the implementation, we usually combine these matrices into a *tensor* (think of it as a 3d array) of shape [n_heads, d_model, d_qkv] to speed up computations.\n","\n","Each attention head will compute contextualized embeddings as in the previous exercise. The main difference is that before the final projection, you will concatenate the outputs of all the heads and project back down to the dimension of the model. \n","\n","So if $\\text{out}_i^j$ is the output at token $i$ by attention head $j$, the combined output is  $y_i = \\text{Concat}(\\text{out}_i^1, \\ldots, \\text{out}_i^{num\\_heads})W_o$\n","\n","Feel free to reuse code from the previous exercise. If you really want to get familiar with manipulating these matrices and tensors, use einsum. You can avoid using any for loops at all. "]},{"cell_type":"code","metadata":{"id":"GGl7yVTZg9rc","executionInfo":{"status":"ok","timestamp":1635572164799,"user_tz":420,"elapsed":165,"user":{"displayName":"Wootae Jeon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl4SKlzSu1Hl1zJoxXC4i2tUYG0XC4OZ5Eso8p=s64","userId":"06138653401797949709"}}},"source":["# parameters for multihead attention\n","n_heads = 8\n","\n","# projection matrices to query, keys, and values\n","W_q = np.load(path+'W_q_mh.npy')\n","W_k = np.load(path+'W_k_mh.npy')\n","W_v = np.load(path+'W_v_mh.npy')\n","\n","# final projection\n","W_o = np.load(path+'W_o_mh.npy')"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"id":"DFjQ0k7Wjozf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635572165885,"user_tz":420,"elapsed":2,"user":{"displayName":"Wootae Jeon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl4SKlzSu1Hl1zJoxXC4i2tUYG0XC4OZ5Eso8p=s64","userId":"06138653401797949709"}},"outputId":"8b9162cf-2d66-4ac4-fa01-3c725eef0a50"},"source":["print(x.shape)\n","print(W_q.shape)\n","print(W_k.shape)\n","print(W_v.shape)\n","print(W_o.shape)\n","\n","def apply_multihead_attention(x, W_q, W_k, W_v, W_o):\n","  \n","  ### YOUR CODE HERE\n","  out_list = np.empty([n_heads, 32, 32])\n","  for h in range(n_heads):\n","    # find Q, K, V\n","    Q = x @ W_q[h]\n","    K = x @ W_k[h]\n","    V = x @ W_v[h]\n","    # compute attention weights\n","    A = Q @ K.T / (d_qkv ** 0.5)\n","    A = softmax(A) \n","    # apply attention to values\n","    weighted = np.einsum('ij,jk->jik', A, V)\n","    out = weighted.sum(axis=0)\n","    out_list[h] = out\n","  # concatenate head outputs and project\n","  y = np.concatenate(out_list, axis=1) @ W_o\n","  return y\n","\n","# expected output\n","y_ = np.load(path + 'y_mh.npy')\n","assert np.allclose(y_, apply_multihead_attention(x, W_q, W_k, W_v, W_o))"],"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["(32, 256)\n","(8, 256, 32)\n","(8, 256, 32)\n","(8, 256, 32)\n","(256, 256)\n"]}]},{"cell_type":"markdown","metadata":{"id":"yURtxc8zgH5M"},"source":["Hint: Keep track of the shapes of matrices that you compute. "]},{"cell_type":"markdown","metadata":{"id":"ULnJ9hB9FHZV"},"source":["### PART II: BERT Fine-Tuning Walkthrough\n","\n","Take a look at [this notebook](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb#scrollTo=TJmV43-aMYPF) that teaches you how to load a pre-trained BERT model and fine-tune it to the task of text classification. Pay *attention* to the workflow and answer the following questions:\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QNT7kmi4PPuq"},"source":["1. What architecture is used on top of BERT as the final classifier?\n","\n","Transformer"]},{"cell_type":"markdown","metadata":{"id":"6F1r0fOrQNCX"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"KCjIbsnJPu_z"},"source":["2. Which function prepares text to be fed into BERT?\n","\n","tokenzier (DistilBertTokenizer.from_pretrained)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"z-4PgUTqQPHX"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"w-9nZ2RsRBl_"},"source":["3. What are the roles of the special tokens [CLS] and [SEP] in BERT's input? You might want to look this up.\n","\n","CLS: represent the start of sequence\n","SEP: separate segment"]},{"cell_type":"markdown","metadata":{"id":"rDUtSNbWRQqw"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"wBHsmJCSQMTK"},"source":["4. What is your intuition for why pre-training improves performance on downstream tasks?\n","\n","reduces training time with pre-training"]},{"cell_type":"markdown","metadata":{"id":"mbX384sUQPis"},"source":["YOUR ANSWER HERE"]}]}